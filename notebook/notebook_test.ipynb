{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\elect\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\elect\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\elect\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "import nltk \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "import gensim\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "nltk.download('stopwords') \n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                               Title  \\\n0  Unable to continue with rebase due to untracke...   \n1  Progressive Web App on iOS 12.2 stuck in offli...   \n2         __PRETTY_FUNCTION__ in constant expression   \n3  How to fix 'http: named cookie not present' in...   \n4  How can I read a file which will be upload fro...   \n\n                                                Body  \\\n0  I'm currently mergin two branches using git re...   \n1  I installed a custom progressive web app via s...   \n2  Please refer to this snippet:\\n\\n#include type...   \n3  I'm building a small dinner/plan management ap...   \n4  I create a method in my .Net Core API which wi...   \n\n                                  Tags  \n0                                 git   \n1            ios progressive-web-apps   \n2       c++ c++17 constant-expression   \n3         http go cookies jwt postman   \n4  c# file upload asp.net-core-webapi   ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Title</th>\n      <th>Body</th>\n      <th>Tags</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Unable to continue with rebase due to untracke...</td>\n      <td>I'm currently mergin two branches using git re...</td>\n      <td>git</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Progressive Web App on iOS 12.2 stuck in offli...</td>\n      <td>I installed a custom progressive web app via s...</td>\n      <td>ios progressive-web-apps</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>__PRETTY_FUNCTION__ in constant expression</td>\n      <td>Please refer to this snippet:\\n\\n#include type...</td>\n      <td>c++ c++17 constant-expression</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>How to fix 'http: named cookie not present' in...</td>\n      <td>I'm building a small dinner/plan management ap...</td>\n      <td>http go cookies jwt postman</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>How can I read a file which will be upload fro...</td>\n      <td>I create a method in my .Net Core API which wi...</td>\n      <td>c# file upload asp.net-core-webapi</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"../data/data_clean.csv\")\n",
    "data = data[[\"Id\", \"Title\", \"Body\", \"Tags\"]]\n",
    "data.reset_index(inplace=True)\n",
    "data.drop(columns='Id', inplace=True)\n",
    "data.drop(columns='index', inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data['Post'] = data.apply(lambda x: (x['Title'] + ' ' + x['Body'] if x['Title'] == x['Title'] else x['Body']).lower(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_73216\\3954808245.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mdata\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'Tokens'\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mapply\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;32mlambda\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mwordpunct_tokenize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'Post'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001B[0m in \u001B[0;36mapply\u001B[1;34m(self, func, axis, raw, result_type, args, **kwargs)\u001B[0m\n\u001B[0;32m   8846\u001B[0m             \u001B[0mkwargs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   8847\u001B[0m         )\n\u001B[1;32m-> 8848\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mop\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mapply\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__finalize__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmethod\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m\"apply\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   8849\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   8850\u001B[0m     def applymap(\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py\u001B[0m in \u001B[0;36mapply\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    731\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mapply_raw\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    732\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 733\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mapply_standard\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    734\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    735\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0magg\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py\u001B[0m in \u001B[0;36mapply_standard\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    855\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    856\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mapply_standard\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 857\u001B[1;33m         \u001B[0mresults\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mres_index\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mapply_series_generator\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    858\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    859\u001B[0m         \u001B[1;31m# wrap results\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py\u001B[0m in \u001B[0;36mapply_series_generator\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    871\u001B[0m             \u001B[1;32mfor\u001B[0m \u001B[0mi\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mv\u001B[0m \u001B[1;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mseries_gen\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    872\u001B[0m                 \u001B[1;31m# ignore SettingWithCopy here in case the user mutates\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 873\u001B[1;33m                 \u001B[0mresults\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mf\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mv\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    874\u001B[0m                 \u001B[1;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mresults\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mABCSeries\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    875\u001B[0m                     \u001B[1;31m# If we have a view on v, we need to make a copy because\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_73216\\3954808245.py\u001B[0m in \u001B[0;36m<lambda>\u001B[1;34m(x)\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mdata\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'Tokens'\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mapply\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;32mlambda\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mwordpunct_tokenize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'Post'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "data['Tokens'] = data.apply(lambda x: wordpunct_tokenize(x['Post']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "most_used_programming_language = [\n",
    "    \"javascript\",\n",
    "    \"js\",\n",
    "    \"python\",\n",
    "    \"py\",\n",
    "    \"go\",\n",
    "    \"golang\",\n",
    "    \"java\",\n",
    "    \"kotlin\",\n",
    "    \"php\",\n",
    "    \"csharp\"\n",
    "    \"c#\",\n",
    "    \"swift\",\n",
    "    \"net\",\n",
    "    \"core\", \n",
    "    \"rb\",\n",
    "    \"ruby\",\n",
    "    \"c\",\n",
    "    \"c++\",\n",
    "    \"cpp\",\n",
    "    \"matlab\",\n",
    "    \"typescript\",\n",
    "    \"ts\",\n",
    "    \"scala\",\n",
    "    \"html\",\n",
    "    \"css\",\n",
    "    \"rust\",\n",
    "    \"rs\",\n",
    "    \"perl\"\n",
    "]\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def token_plot(tokens):\n",
    "    token_count = {\n",
    "        \"javascript\": 0,\n",
    "        \"js\": 0,\n",
    "        \"python\": 0,\n",
    "        \"py\": 0,\n",
    "        \"go\": 0,\n",
    "        \"golang\": 0,\n",
    "        \"java\": 0,\n",
    "        \"kotlin\": 0,\n",
    "        \"php\": 0,\n",
    "        \"csharp\": 0,\n",
    "        \"c#\": 0,\n",
    "        \"swift\": 0,\n",
    "        \"net\": 0,\n",
    "        \"core\": 0,\n",
    "        \"rb\": 0,\n",
    "        \"ruby\": 0,\n",
    "        \"c\": 0,\n",
    "        \"c++\": 0,\n",
    "        \"cpp\": 0,\n",
    "        \"matlab\": 0,\n",
    "        \"typescript\": 0,\n",
    "        \"ts\": 0,\n",
    "        \"scala\": 0,\n",
    "        \"html\": 0,\n",
    "        \"css\": 0,\n",
    "        \"rust\": 0,\n",
    "        \"rs\": 0,\n",
    "        \"perl\": 0\n",
    "    }\n",
    "\n",
    "    for token_series in tokens:\n",
    "        for token in token_series:\n",
    "            if token in most_used_programming_language:\n",
    "                token_count[token] += 1\n",
    "\n",
    "    df = pd.DataFrame.from_dict(token_count, orient='index')\n",
    "    df.plot(kind='bar', color=\"#f56900\", title='Top des langages de programmation les plus cités')\n",
    "\n",
    "token_plot(data['Tokens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def delete_stopwords(tokens):\n",
    "    return [token for token in tokens if token not in stop_words]   \n",
    "     \n",
    "data['Tokens'] = data.apply(lambda x: delete_stopwords(x['Tokens']), axis=1)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "token_plot(data['Tokens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Lemmatize"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def lemmatize(token_series):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = []\n",
    "    for token in token_series:\n",
    "        tokens.append(lemmatizer.lemmatize(token))\n",
    "    return tokens\n",
    "\n",
    "data['TokensLem'] = data.apply(lambda x: lemmatize(x['Tokens']), axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Bag Of Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def bag_of_word(x):\n",
    "    cv = CountVectorizer(stop_words='english')\n",
    "    bog = cv.fit_transform(x['Tokens'])\n",
    "    return bog\n",
    "\n",
    "def bag_of_word_lem(x):\n",
    "    cv = CountVectorizer(stop_words='english')\n",
    "    bog_lem = cv.fit_transform(x['TokensLem'])\n",
    "    return bog_lem\n",
    "\n",
    "\n",
    "data['BOG'] = data.apply(lambda x: bag_of_word(x), axis=1)\n",
    "data['BogLem'] = data.apply(lambda x: bag_of_word_lem(x), axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()  \n",
    "\n",
    "data['Tfidt'] = data.apply(lambda x: tfidf.fit_transform(x['TokensLem']), axis=1)\n",
    "\n",
    "print(tfidf.get_feature_names())\n",
    "data['Tokens'].head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# World2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#w2v_size=300\n",
    "#w2v_window=5\n",
    "#w2v_min_count=1\n",
    "#w2v_epochs=100\n",
    "#maxlen = len(data['Post']) # adapt to length of sentences\n",
    "sentences = data['Post'].to_list()\n",
    "sentences = [gensim.utils.simple_preprocess(text) for text in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from tensorflow.python.distribute.multi_process_runner import multiprocessing\n",
    "#print(\"Build & train Word2Vec model ...\")\n",
    "#w2v_model = gensim.models.Word2Vec(min_count=w2v_min_count, window=w2v_window, vector_size=w2v_size, seed=42, workers=1)\n",
    "#workers = multiprocessing.cpu_count()\n",
    "#w2v_model.build_vocab(sentences)\n",
    "#w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=w2v_epochs)\n",
    "#model_vectors = w2v_model.wv\n",
    "#w2v_words = model_vectors.index_to_key\n",
    "#print(\"Vocabulary size: %i\" % len(w2v_words))\n",
    "#print(\"Word2Vec trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sim_words = w2v_model.wv.most_similar('cpp')\n",
    "\n",
    "#print(sim_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sim_words = w2v_model.wv.most_similar(positive=[\"gcc\", \"cpp\"], negative=\"js\")\n",
    "#print(sim_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get cosine similairty matrix\n",
    "#def cos_sim(input_vectors):\n",
    "#    similarity = cosine_similarity(input_vectors)\n",
    "#    return similarity\n",
    "\n",
    "# get topN similar sentences\n",
    "#def get_top_similar(index, sentence_list, similarity_matrix, topN):\n",
    "    # get the corresponding row in similarity matrix\n",
    "#   similarity_row = np.array(similarity_matrix[index, :])\n",
    "    # get the indices of top similar\n",
    "#    indices = similarity_row.argsort()[-topN:][::-1]\n",
    "#    return [sentence_list[i] for i in indices]\n",
    "\n",
    "#embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "\n",
    "#sentences_embeddings = embed(data['Post'])\n",
    "\n",
    "#similarity_matrix = cos_sim(np.array(sentences_embeddings))\n",
    "\n",
    "#top_similar = get_top_similar(0, data['Post'], similarity_matrix, 3)\n",
    "\n",
    "#for x in range(len(top_similar)):\n",
    "#    print(\"----\")\n",
    "#    print(top_similar[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#BERT_MODEL = \"https://tfhub.dev/google/experts/bert/wiki_books/2\"\n",
    "#PREPROCESS_MODEL = \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\"\n",
    "#import tensorflow_text\n",
    "#preprocess = hub.load(PREPROCESS_MODEL)\n",
    "#bert = hub.load(BERT_MODEL)\n",
    "#bert_inputs = preprocess(data['Post'].head(150))\n",
    "\n",
    "#bert_outputs = bert(bert_inputs, training=False)\n",
    "#pooled_output = bert_outputs['pooled_output']\n",
    "#sequence_output = bert_outputs['sequence_output']\n",
    "\n",
    "#print('\\nSentences:')\n",
    "#print(data['Post'][0])\n",
    "#print('\\nPooled output:')\n",
    "#print(pooled_output[0])\n",
    "#print('\\nSequence output:')\n",
    "#print(sequence_output[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def tags_to_array(x):\n",
    "    tags = []\n",
    "    for tag in x['Tags'].split(\" \"):\n",
    "        if tag != \" \" or tag != \" \":\n",
    "            tags.append(tag)\n",
    "    return tags\n",
    "\n",
    "data['Tags'] = data.apply(lambda x: tags_to_array(x), axis=1)\n",
    "data.dropna(inplace=True, axis=1)\n",
    "data.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# LDA"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "def make_pca(x):\n",
    "    pca = TruncatedSVD(n_components=2)\n",
    "    X_reduced_train = pca.fit_transform(x['Tfidt'])\n",
    "    return X_reduced_train\n",
    "\n",
    "data['Tfidt_PCA'] = data.apply(lambda x: make_pca(x), axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from gensim.models import TfidfModel\n",
    "from gensim import corpora, models\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "def make_lda(posts, num_topic):\n",
    "    dictionary = corpora.Dictionary(posts)\n",
    "    dictionary.filter_extremes(no_below=1000)\n",
    "    bow_corpus = [dictionary.doc2bow(text) for text in posts]\n",
    "    tfidf = TfidfModel(bow_corpus)\n",
    "    tfidf_corpus = [tfidf[text] for text in bow_corpus]\n",
    "\n",
    "    ldamodel = gensim.models.ldamulticore.LdaMulticore(tfidf_corpus, num_topics=num_topic, id2word = dictionary, passes=20)\n",
    "    coherencemodel = CoherenceModel(model=ldamodel, texts=posts, dictionary=dictionary, coherence='c_v')\n",
    "    return coherencemodel.get_coherence()\n",
    "\n",
    "\n",
    "score_lda = []\n",
    "for i in range(1, 51):\n",
    "    score_lda.append(make_lda(sentences, i))\n",
    "\n",
    "print(score_lda)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "limit=52; start=2; step=1;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, score_lda)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(sentences)\n",
    "dictionary.filter_extremes(no_below=1000)\n",
    "bow_corpus = [dictionary.doc2bow(text) for text in sentences]\n",
    "tfidf = TfidfModel(bow_corpus)\n",
    "tfidf_corpus = [tfidf[text] for text in bow_corpus]\n",
    "\n",
    "ldamodel = gensim.models.ldamulticore.LdaMulticore(tfidf_corpus, num_topics=7, id2word = dictionary, passes=20)\n",
    "\n",
    "def predict_unsupervised_tags(text):\n",
    "    corpus_new = dictionary.doc2bow(text)\n",
    "    topics = ldamodel.get_document_topics(corpus_new)\n",
    "\n",
    "    relevant_topic = topics[0][0]\n",
    "    relevant_topic_prob = topics[0][1]\n",
    "\n",
    "    for i in range(len(topics)):\n",
    "        if topics[i][1] > relevant_topic_prob:\n",
    "            relevant_topic = topics[i][0]\n",
    "            relevant_topic_prob = topics[i][1]\n",
    "\n",
    "    potential_tags = ldamodel.get_topic_terms(topicid=relevant_topic, topn=20)\n",
    "\n",
    "    relevant_tags = [dictionary[tag[0]] for tag in potential_tags if dictionary[tag[0]] in text]\n",
    "\n",
    "    return relevant_tags\n",
    "\n",
    "sentences = data['Posts']\n",
    "sentences = [gensim.utils.simple_preprocess(text) for text in sentences]\n",
    "\n",
    "print(predict_unsupervised_tags(sentences))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "0a9fc5520393de83a7f52d6621ffb24ff74d759d9b38ad074dbe17a18af8c6a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
