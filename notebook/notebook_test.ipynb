{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook de test \n",
    "\n",
    "Dans ce notebook, nous allons tester les différents modèles qui pourraient être intéressant pour prédire les tags associés à une question de stackoverflow.\n",
    "\n",
    "Dans un premier temps, nous importons à notre habitude les librairies nécessaires à nos tests de modélisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "import nltk \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "import gensim\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tensorflow.python.distribute.multi_process_runner import multiprocessing\n",
    "from matplotlib import pyplot as plt\n",
    "from gensim.models import TfidfModel\n",
    "from gensim import corpora, models\n",
    "from gensim.models import CoherenceModel\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import pyLDAvis.gensim_models\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "nltk.download('stopwords') \n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons désormais lire notre dataset et garder uniquement les colonnes qui nous intéressent à savoir Title, Body et Tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/data_clean.csv\")\n",
    "data = data[[\"Title\", \"Body\", \"Tags\"]]\n",
    "data.reset_index(inplace=True)\n",
    "data.drop(columns='index', inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, nous allons constituer notre corpus qui sera créé à partir du titre, du corps et des différents tags pour chaque document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data['Post'] = data.apply(lambda x: (x['Title'] + ' ' + x['Body'] + ' ' + x['Tags'] if x['Title'] == x['Title'] else x['Body']).lower(), axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons maintenant décomposer notre corpus en tokens grâce à la méthode wordpunc qui permettra également de supprimer la ponctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data['Tokens'] = data.apply(lambda x: wordpunct_tokenize(x['Post']), axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vérifions le nombre d'occurrences à des langages de programmation diffère dans l'ensemble de posts. Nous pourrions afficher un graphique l'illustrant et mieux comprendre la proportion d'utilisation des langages de programmation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "most_used_programming_language = [\n",
    "    \"javascript\",\n",
    "    \"js\",\n",
    "    \"python\",\n",
    "    \"py\",\n",
    "    \"go\",\n",
    "    \"golang\",\n",
    "    \"java\",\n",
    "    \"kotlin\",\n",
    "    \"php\",\n",
    "    \"csharp\"\n",
    "    \"c#\",\n",
    "    \"swift\",\n",
    "    \"net\",\n",
    "    \"core\", \n",
    "    \"rb\",\n",
    "    \"ruby\",\n",
    "    \"c\",\n",
    "    \"c++\",\n",
    "    \"cpp\",\n",
    "    \"matlab\",\n",
    "    \"typescript\",\n",
    "    \"ts\",\n",
    "    \"scala\",\n",
    "    \"html\",\n",
    "    \"css\",\n",
    "    \"rust\",\n",
    "    \"rs\",\n",
    "    \"perl\"\n",
    "]\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def token_plot(tokens):\n",
    "    token_count = {\n",
    "        \"javascript\": 0,\n",
    "        \"js\": 0,\n",
    "        \"python\": 0,\n",
    "        \"py\": 0,\n",
    "        \"go\": 0,\n",
    "        \"golang\": 0,\n",
    "        \"java\": 0,\n",
    "        \"kotlin\": 0,\n",
    "        \"php\": 0,\n",
    "        \"csharp\": 0,\n",
    "        \"c#\": 0,\n",
    "        \"swift\": 0,\n",
    "        \"net\": 0,\n",
    "        \"core\": 0,\n",
    "        \"rb\": 0,\n",
    "        \"ruby\": 0,\n",
    "        \"c\": 0,\n",
    "        \"c++\": 0,\n",
    "        \"cpp\": 0,\n",
    "        \"matlab\": 0,\n",
    "        \"typescript\": 0,\n",
    "        \"ts\": 0,\n",
    "        \"scala\": 0,\n",
    "        \"html\": 0,\n",
    "        \"css\": 0,\n",
    "        \"rust\": 0,\n",
    "        \"rs\": 0,\n",
    "        \"perl\": 0\n",
    "    }\n",
    "\n",
    "    for token_series in tokens:\n",
    "        for token in token_series:\n",
    "            if token in most_used_programming_language:\n",
    "                token_count[token] += 1\n",
    "\n",
    "    df = pd.DataFrame.from_dict(token_count, orient='index')\n",
    "    df.plot(kind='bar', color=\"#f56900\", title='Top des langages de programmation les plus cités')\n",
    "\n",
    "token_plot(data['Tokens'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bien, nous avons une bonne idée des principaux éléments et du champ lexical qui pourrait être présent.\n",
    "\n",
    "À première vue, le langage sera très performant pour comprendre le champ lexical autour du C et du Java est sûrement beaucoup moins sûr celui de Rust par exemple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# StopWords"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons utiliser le vocabulaire anglais et la le stopwords de ntlk pour supprimer l'ensemble des stopwords. Puis appliquons le sur l'ensemble des tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def delete_stopwords(tokens):\n",
    "    return [token for token in tokens if token not in stop_words]   \n",
    "     \n",
    "data['Tokens'] = data.apply(lambda x: delete_stopwords(x['Tokens']), axis=1) \n",
    "\n",
    "data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vérifions la proportion des langages de programmation pour s'assurer que les langages de programmation n'ont pas été supprimer par le stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "token_plot(data['Tokens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Lemmatize"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appliquons désormais un processus de lemmatisation sur nos tokens pour ne garder que l'infinitif des verbes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lemmatize_series(token_series):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = []\n",
    "    for token in token_series:\n",
    "        tokens.append(lemmatizer.lemmatize(token))\n",
    "    return tokens\n",
    "\n",
    "def lemmatize_str(str): \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return lemmatizer.lemmatize(str) \n",
    "\n",
    "data['TokensLem'] = data.apply(lambda x: lemmatize_series(x['Tokens']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Bag Of Word"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appliquons maintenant un Bag of word et un TF-IDT à nos Tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def bag_of_word(x):\n",
    "    cv = CountVectorizer(stop_words='english')\n",
    "    bow = cv.fit_transform(x['Tokens'])\n",
    "    return bow \n",
    "\n",
    "data['BOW'] = data.apply(lambda x: bag_of_word(x), axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()  \n",
    "\n",
    "data['Tfidt'] = data.apply(lambda x: tfidf.fit_transform(x['TokensLem']), axis=1)\n",
    "\n",
    "print(tfidf.get_feature_names())\n",
    "data['Tokens'].head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous voyons ici les principales features trouvées par le Tf IDT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implémentons notre premier modèle le Word2Vec :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w2v_size=300\n",
    "w2v_window=5\n",
    "w2v_min_count=1\n",
    "w2v_epochs=100\n",
    "maxlen = len(data['Post']) # adapt to length of sentences\n",
    "sentences = data['Post'].to_list()\n",
    "sentences = [gensim.utils.simple_preprocess(text) for text in sentences]\n",
    "\n",
    "print(\"Build & train Word2Vec model ...\")\n",
    "workers = multiprocessing.cpu_count()\n",
    "w2v_model = gensim.models.Word2Vec(min_count=w2v_min_count, window=w2v_window, vector_size=w2v_size, seed=42, workers=workers)\n",
    " \n",
    "w2v_model.build_vocab(data['Tokens'])\n",
    "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=w2v_epochs)\n",
    "model_vectors = w2v_model.wv\n",
    "w2v_words = model_vectors.index_to_key  \n",
    "print(\"Vocabulary size: %i\" % len(w2v_words))\n",
    "print(\"Word2Vec trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = \"I the best developer in the world but i destroy my git commit history with rebase for cpp code, can u help me ?\"\n",
    "s = s.lower()\n",
    "s = lemmatize_str(s)\n",
    "s = wordpunct_tokenize(s)\n",
    "s = delete_stopwords(s)\n",
    "s = w2v_model.wv.most_similar(s)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Use"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implémentation du modèle USE : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cos_sim(input_vectors):\n",
    "    similarity = cosine_similarity(input_vectors)\n",
    "    return similarity\n",
    "\n",
    "def get_top_similar(index, sentence_list, similarity_matrix, topN):\n",
    "   similarity_row = np.array(similarity_matrix[index, :])\n",
    "   indices = similarity_row.argsort()[-topN:][::-1]\n",
    "   return [sentence_list[i] for i in indices]\n",
    "\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "\n",
    "sentences_embeddings = embed(data['Post'])\n",
    "\n",
    "similarity_matrix = cos_sim(np.array(sentences_embeddings))\n",
    "\n",
    "top_similar = get_top_similar(0, data['Post'], similarity_matrix, 3)\n",
    "\n",
    "for x in range(len(top_similar)):\n",
    "    print(\"----\")\n",
    "    print(top_similar[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Bert"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implémentons le modèle BERT : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BERT_MODEL = \"https://tfhub.dev/google/experts/bert/wiki_books/2\"\n",
    "PREPROCESS_MODEL = \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\"\n",
    "import tensorflow_text\n",
    "preprocess = hub.load(PREPROCESS_MODEL)\n",
    "bert = hub.load(BERT_MODEL)\n",
    "bert_inputs = preprocess(data['Post'].head(150))\n",
    "\n",
    "bert_outputs = bert(bert_inputs, training=False)\n",
    "pooled_output = bert_outputs['pooled_output']\n",
    "sequence_output = bert_outputs['sequence_output']\n",
    "\n",
    "print('\\nSentences:')\n",
    "print(data['Post'][0])\n",
    "print('\\nPooled output:')\n",
    "print(pooled_output[0])\n",
    "print('\\nSequence output:')\n",
    "print(sequence_output[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Réduction dimensionnelle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons appliquer une réduction dimensionnelle à notre TfIdt : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pca(x):\n",
    "    pca = TruncatedSVD(n_components=2)\n",
    "    X_reduced_train = pca.fit_transform(x['Tfidt'])\n",
    "    return X_reduced_train\n",
    "\n",
    "data['Tfidt_PCA'] = data.apply(lambda x: make_pca(x), axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos tests ne semblent pas concluant avec Word2Vec, Bert et USE... Nous allons essayer d'implémenter une LDA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def make_lda(posts, num_topic):\n",
    "    dictionary = corpora.Dictionary(posts)\n",
    "    dictionary.filter_extremes(no_below=1000)\n",
    "    bow_corpus = [dictionary.doc2bow(text) for text in posts]\n",
    "    tfidf = TfidfModel(bow_corpus)\n",
    "    tfidf_corpus = [tfidf[text] for text in bow_corpus]\n",
    "\n",
    "    ldamodel = gensim.models.ldamodel.LdaModel(tfidf_corpus, num_topics=num_topic, id2word = dictionary, passes=20)\n",
    "    coherencemodel = CoherenceModel(model=ldamodel, texts=posts, dictionary=dictionary, coherence='c_v')\n",
    "    return coherencemodel.get_coherence()\n",
    "    \n",
    "\n",
    "score_lda = []\n",
    "for i in range(1, 16):\n",
    "    score_lda.append(make_lda(sentences, i))\n",
    "\n",
    "print(score_lda)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le tableau de score semble plus intéressant que les précédents modèles.\n",
    "\n",
    "Analysons ses informations sur une courbe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "limit=17; start=2; step=1\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, score_lda)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons voir que le nombre idéal de topic est autour de 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(sentences)\n",
    "dictionary.filter_extremes(no_below=1000)\n",
    "bow_corpus = [dictionary.doc2bow(text) for text in sentences]\n",
    "tfidf = TfidfModel(bow_corpus)\n",
    "tfidf_corpus = [tfidf[text] for text in bow_corpus]\n",
    "\n",
    "ldamodel = gensim.models.ldamulticore.LdaMulticore(tfidf_corpus, num_topics=7, id2word = dictionary, passes=20)\n",
    "lda_visualization = pyLDAvis.gensim_models.prepare(ldamodel, tfidf_corpus, dictionary, sort_topics=False)\n",
    "\n",
    "topics = ldamodel.get_document_topics(bow_corpus) #\n",
    "pyLDAvis.display(lda_visualization) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_lda(x):\n",
    "    bow_vector = dictionary.doc2bow(x['Tokens'])\n",
    "    return ldamodel.get_document_topics(bow_vector)\n",
    " \n",
    "doc_topic = predict_lda(data.iloc[9]) \n",
    "alL_topic = ldamodel.get_topics() \n",
    "\n",
    "for n, t in doc_topic:\n",
    "    topic_most_pr = alL_topic[n].argmax()\n",
    "    print(\"doc: {} topic: {}\\n prob : \".format(n, topic_most_pr, t))\n",
    "    topic_name = ldamodel.print_topic(n, 5) \n",
    "    print(topic_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBOW"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implémentation du modèle CBow : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [ ' '.join(token) for token in data['Tokens'].head(100) ]  \n",
    "words = ' '.join(words) \n",
    "words = words.split(\" \")\n",
    "print(words[0])\n",
    "print(words[1])\n",
    "vocab = set(words)\n",
    "vocab_size = len(vocab)\n",
    "embed_dim = 10\n",
    "context_size = 2\n",
    "\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "ix_to_word = {i: word for i, word in enumerate(vocab)}\n",
    "\n",
    "data = []\n",
    "for i in range(2, len(words) - 2):\n",
    "    context = [words[i - 2], words[i - 1], words[i + 1], words[i + 2]]\n",
    "    target = words[i]\n",
    "    data.append((context, target)) \n",
    "print(data[:5])\n",
    "\n",
    "embeddings =  np.random.random_sample((vocab_size, embed_dim))\n",
    "\n",
    "def linear(m, theta):\n",
    "    w = theta\n",
    "    return m.dot(w)\n",
    "\n",
    "def log_softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return np.log(e_x / e_x.sum())\n",
    "\n",
    "def NLLLoss(logs, targets):\n",
    "    out = logs[range(len(targets)), targets]\n",
    "    return -out.sum()/len(out)\n",
    "\n",
    "def log_softmax_crossentropy_with_logits(logits,target):\n",
    "\n",
    "    out = np.zeros_like(logits)\n",
    "    out[np.arange(len(logits)),target] = 1\n",
    "    \n",
    "    softmax = np.exp(logits) / np.exp(logits).sum(axis=-1,keepdims=True)\n",
    "    \n",
    "    return (- out + softmax) / logits.shape[0]\n",
    "\n",
    "def forward(context_idxs, theta):\n",
    "    m = embeddings[context_idxs].reshape(1, -1)\n",
    "    n = linear(m, theta)\n",
    "    o = log_softmax(n)\n",
    "    \n",
    "    return m, n, o\n",
    "\n",
    "def backward(preds, theta, target_idxs):\n",
    "    m, n, o = preds\n",
    "    \n",
    "    dlog = log_softmax_crossentropy_with_logits(n, target_idxs)\n",
    "    dw = m.T.dot(dlog)\n",
    "    \n",
    "    return dw\n",
    "\n",
    "def optimize(theta, grad, lr=0.03):\n",
    "    theta -= grad * lr\n",
    "    return theta\n",
    "\n",
    "theta = np.random.uniform(-1, 1, (2 * context_size * embed_dim, vocab_size))\n",
    "\n",
    "epoch_losses = {}\n",
    "\n",
    "for epoch in range(80):\n",
    "\n",
    "    losses =  []\n",
    "\n",
    "    for context, target in data:\n",
    "        context_idxs = np.array([word_to_ix[w] for w in context])\n",
    "        preds = forward(context_idxs, theta)\n",
    "\n",
    "        target_idxs = np.array([word_to_ix[target]])\n",
    "        loss = NLLLoss(preds[-1], target_idxs)\n",
    "\n",
    "        losses.append(loss)\n",
    "\n",
    "        grad = backward(preds, theta, target_idxs)\n",
    "        theta = optimize(theta, grad, lr=0.03)\n",
    "        \n",
    "     \n",
    "    epoch_losses[epoch] = losses \n",
    "\n",
    "ix = np.arange(0,80)\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Epoch/Losses', fontsize=20)\n",
    "plt.plot(ix,[epoch_losses[i][0] for i in ix])\n",
    "plt.xlabel('Epochs', fontsize=12)\n",
    "plt.ylabel('Losses', fontsize=12) \n",
    "\n",
    "def predict(words):\n",
    "    context_idxs = np.array([word_to_ix[w] for w in words])\n",
    "    preds = forward(context_idxs, theta)\n",
    "    word = ix_to_word[np.argmax(preds[-1])]\n",
    "    \n",
    "    return word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
